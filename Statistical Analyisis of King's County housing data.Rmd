---
title: "STAT 425 Project - Statistical Analysis of King County Housing Data"
author: "Sreekanth Krishnaiah, Pak On Chan"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE)
```

```{R, echo=FALSE}
library(car)
attach(mtcars)
library(MASS)
library(dplyr)
library(ggplot2)
```


#1. Introduction

In this project, we study the King County housing market from the King County housing dataset. The objective is to determine the relationship between the house price in King County and the house features through statistical inference. The content of the report is organised as follows. In section 2, we work on the King County housing dataset. We explore and explain the dataset. Also, we preprocess and tranform the data for statistical analysis. In section 3, we present our first result.  We construct a linear regression model which explain the house price in King County. From the model, we identify the significant features which determine the house price. In section 4, we present our second result. We study the pairwise difference in the mean house price from different groups, from which we gives recommendations in boosting the house price. In section 5, we present our third result. We suggest the factors that can increase the selling price of a house in affluent and poor areas. Besides, a data visualization dashboard has also been constructed to better visualize the data.



#2. Data Exploration and Preprocessing


###2.1 Dataset

In this project, we work on the King County housing dataset. King County located in Seattle of Washington. It is the most populous county in Washington, where the population is about 2100000. Here, we focus on the King County housing data available in https://www.kaggle.com/harlfoxem/housesalesprediction. In the King County housing dataset, there are n=21613 samples of residential houses. Each sample is labelled by its house price together with 20 features:

```{R, echo=FALSE}
cat("All features")
print(c("id", "date", "bedrooms", "bathrooms", "sqft_living", "sqft_lot", "floors", "waterfront", "view", "condition", "grade", "sqft_above", "sqft_basement", "yr_built", "yr_renovated", "zipcode", "lat", "long", "sqft_living15", "sqft_lot15"))
```

Below is a brief description of the features in the King County dataset:

1. id - Each house in the county is given a unique ID.

2. date - Date at which the house was sold.

3. bedrooms - No. of bedrooms in the house.

4. bathrooms - No. of bathrooms per bedroom.

5. sqft_living - The total square footage of the house

6. sqft_lot - Lot size of the house.

7. floors - No. of floors in the house.

8. waterfront - Indication of whether the house has a view to waterfront.

9. view - Indication of whether the house has been viewed.

10. condition - Rating of the overall condition.

11. grade - Rating of the oerall grade.

12. sqft_above - Square footage of the house apart from basement

13. sqft_basement - Size of the basement

14. yr_built - The year in which it was built.

15. yr_renovated - The year in which it was renovated.

16. zipcode - Postal code of the house.

17. lat - The latitude location of the house.

18. long - The longitude location of the house.

19.  sqft_living15 - The house square footage in 2015.

20. sqft_lot15 - The lot square footage in 2015.


###2.2 Data Preprocess

Here, we are going to preprocess the King county housing dataset. As part of the preprocessing of the data, the following tasks have been performed:

1. We eliminate the "id" which does not have much meaning.

2. We code "date" to an ordinal numerical feature "time" in unit of year.

3. We replace the value of "bedrooms" of a sample with an extodinary value 33 by the median.

4. We eliminate "sqft_living" due to the exact collinearity "sqft_living"="sqft_above"+"sqft_basement".

5. We code "floors" as a categorical feature with integral levels 1,2,3.

6. We add a categorical feature "addhalffloor" to indicate if 0.5 floor is added on the house.

7. We add a categorical feature "basement" to indicate if a basment is built.

8. We code "yr_renovated" as a categorical feature "renovated" to indicate if the house is renovated.

```{R, echo=FALSE}
data = read.csv("kc_house_data.csv", header = TRUE)
n = dim(data)[1]
data[,"id"] = NULL
data$time = as.numeric(substr(data$date, 1, 4))+as.numeric(substr(data$date, 5, 6))/12
data[,"date"] = NULL
data$bedrooms=ifelse(data$bedrooms==33,median(data$bedrooms),data$bedrooms)
data[,"sqft_living"] = NULL
addhalffloor = as.factor(ifelse(data$floors-floor(data$floors)==0,0,1))
data$addhalffloor = as.factor(ifelse(data$floors-floor(data$floors)==0,0,1))
integralfloor = floor(data$floors)
data$floors = as.factor(floor(data$floors))
data$basement = as.factor(ifelse(data$sqft_basement==0,0,1))
data$renovated = as.factor(ifelse(data$yr_renovated == 0, 0, 1)) 
data[,"yr_renovated"] = NULL
data$view = as.factor(data$view)
data$condition = as.factor(data$condition)
data$grade = as.factor(data$grade)
data$zipcode = as.factor(data$zipcode)

data_bedroom = data
```

After the data preprocessing, there are totally 20 features, in which there are 11 numerical features:

```{R, echo=FALSE}
cat("Numerical features")
print(c("bedrooms", "bathrooms", "sqft_lot", "sqft_above", "sqft_basement", "yr_built", "lat", "long", "sqft_living15", "sqft_lot15", "time"))
```

and 9 categorical features:
```{R, echo=FALSE}
cat("Categorical features")
print(c("floors", "waterfront", "view", "condition", "grade", "zipcode", "addhalffloor", "basement", "renovated"))
```



###2.3 Data Transformation

Here, we perform appropriate transformation on the housing data. On looking at the distribution of the price, it is quite evident that it is non-linear and is of non-constant varaince. In order to achieve the linearity and constant variance, we perform a box-cox transformation on the house price.


The MLE for the Box-Cox parameter is $\lambda$=0.07. So we simply take the closest integer value $\lambda$=0, which is to take log on the price.
```{R, fig.width = 10, fig.height = 4, echo=FALSE}
boxcox(lm(price~.,data=data),lambda=seq(-1, 1, 0.1))
title(main="log-likelihood vs lambda parameter")
```
<br />
Below, we show the changes of house price against several variables before and after the logarithmic transformation. Better linearity and non constant varaince are achieved after the Box-Cox transformation. We will stick to this logorithmic price in the remining of the discussion.
```{R, fig.width = 10, fig.height = 6, echo=FALSE}
par(mfrow=c(2,4))
plot(data$bedrooms,data$price,xlab="bedrooms",ylab="price",main="price vs bedrooms", pch=20)
plot(data$bathrooms,data$price,xlab="bathrooms",ylab="price",main="price vs bathrooms", pch=20)
plot(data$sqft_above,data$price,xlab="sqft_above",ylab="price",main="price vs sqft_above", pch=20)
plot(data$sqft_basement,data$price,xlab="sqft_basement",ylab="price",main="price vs sqft_basement", pch=20)

plot(data$bedrooms,log(data$price),xlab="bedrooms",ylab="log(price)",main="log(price) vs bedrooms", pch=20)
plot(data$bathrooms,log(data$price),xlab="bathrooms",ylab="log(price)",main="log(price) vs bathrooms", pch=20)
plot(data$sqft_above,log(data$price),xlab="sqft_above",ylab="log(price)",main="log(price) vs sqft_above" , pch=20)
plot(data$sqft_basement,log(data$price),xlab="sqft_basement",ylab="log(price)",main="log(price) vs sqft_basement" , pch=20)
```


##3. Result I: The Model for House Price

Here, we are going to get at a linear regression model which explain the house price. Once we arrive at the linear regression model, we will explain how the features in the model affect the house price. We carry out the following procedure to get at our model:

1. We eliminate highly correlted variables from the VIF. Below shows the VIF of the features:

```{R, echo=FALSE}
vif(lm(log(price)~.,data=data))
data[,"long"] = NULL
data[,"lat"] = NULL
```

Highly correlated variables generally exhibit VIF >> 1. The VIF above shows that "zipcode", "long" and "lat" are highly correlated, which is as expected since "zipcode" encodes the location information of "long" and "lat". Since "zipcode" is more informative than "long" and "lat", we choose to eliminate the features "long" and "lat" in the later analysis.


2. We check for the relevant numerical features by the T-test. We perform a temporary linear regression on the housing dataset. The p-value for the T-statistics of "bedrooms" and "sqft_lot15" are given below:

```{R, echo=FALSE}
cat("p-values")
summary(lm(log(price)~.,data=data))$coefficients[,4][c("bedrooms","sqft_lot15")]
data[,"bedrooms"] = NULL
data[,"sqft_lot15"] = NULL
```
<br />
Note that both of the p-values > 0.05, meaning that they do not have significant linear relation witht the logged price. Hence, we eliminate the features "bedrooms" and "sqft_lot15". While it can be checked that marginally the logged price of a house increases with the number of bedrooms, the feature does not shows to be significant in the multiple regression model because "bedrooms" is correlated with other variables like "bathrooms" and "sqft_living".

3. We check for the relevant categorical features by the F-test. We again perform a temporary linear regression. Then we perform the anova on the temporary model. The anova table is given as below:
```{R, echo=FALSE}
anova(lm(log(price)~.,data=data))
```
<br />
The F-statistics of all features has value < 0.05. Hence, we do not do further elimination on the categorical features.


4. We perform the stepwise feature selection on top the previous feature selection based on the hypothesis testing. More precisely, we perform the stepwise model selection with AIC and BIC. The feature selection with AIC do not further eliminate any feature whereas the BIC eliminate the feature "addhalffloor" in the selection. Note that the feature "addhalffloor" also does not show to be very significant in the previous F-test. Nevertheless, intuitively "addhalffloor" should contribute to the house price. So we adopt the selection by AIC where no feature is eliminated.
```{R, echo=FALSE}
#fit_AIC= step(lm(log(price)~.,data=data), direction="both",k=2)
#fit_BIC = step(lm(log(price)~.,data=data), direction="both",k=log(n))
```



5. We perform diagnostic checking on our model.
```{R, fig.width = 10, fig.height = 6, echo=FALSE, warning=FALSE}
fit = lm(log(price)~.,data=data)
par(mfrow=c(2,2))
plot(fit, pch=20)

p = n - df.residual(fit)
leverages = influence(fit)$hat
cat("Number of high leverage points = ", length(leverages[leverages>2*p/n]))

sr = rstudent(fit)
cat("Number of outliers = ", length(sr[abs(sr)>qt(1-0.05/(2*n),df.residual(fit))]))

cook = cooks.distance(fit)
cat("Number of influential points = ", length(cook[cook>1]))
```
<br />
It can be seen that the constant variance and the normality is approximately valid. Besides, from the analysis of leverages, we see that there are 1188 high leverage points. From the T-test of the studentized residuals, we see that there are 35 outliers. From the cook's distance, we see that there is 1 influential point.

The result here is that we get a linear regression model that explain the King County house price with 16 significant features. Within the 16 significant features in the linear regression model, 7 features are numerical features:
```{R, echo=FALSE}
cat("Numerical features")
print(c("bathrooms", "sqft_lot", "sqft_above", "sqft_basement", "yr_built", "sqft_living15", "time"))
```

whereas the remaining 9 features are categorical features:
```{R, echo=FALSE}
cat("Categorical features")
print(c("floors", "waterfront", "view", "condition", "grade", "zipcode", "addhalffloor", "basement", "renovated"))
```
where "waterfront", "addhalffloor", "basement" and "renovated" takes only on values 0 or 1.
The linear regression model explains a large fraction of variance of the logged price with $R_{adj}^2=0.881$. The large $R_{adj}^2$ for the linear regression model is visualized by the high correlation between the logged price $y$ and the estimated logged price $\hat{y}$ shown below
```{R, fig.width = 10, fig.height = 6, echo=FALSE}
y_hat = predict (fit, data = data)
plot(log(data$price),y_hat,xlab="y",ylab="y hat",main="Estimated y vs True y", pch=20)
```

Now, we interpret the coefficients in the linear regression model. For the numerical features, the coefficients are given by
```{R, fig.width = 8, fig.height = 5, echo=FALSE}
cat("Coefficients of numerical features")
signif(fit$coefficients[c("bathrooms", "sqft_lot", "sqft_above", "sqft_basement", "yr_built", "sqft_living15", "time")],3)
```
The coefficients of "bathrooms", "sqft_lot", "sqft_above", "sqft_basement" and "sqft_living15" are positive because they measure the house quality. The coefficients of "time" is positive because the house price increase with time. The negative coefficient for "yr_built" is counter intuitive since the house price should decrease as its age increases. Such negativity can be explained by the "yr_built" and other features. Besides, the comparsion of magnitudes of coefficients is meanningful for "sqft_above","sqft_living15" and "sqft_lot" since they are of the same unit. The order of magnitudes of coefficients is given by "sqft_above" > "sqft_living15" > "sqft_lot". Such order indicates their important in determining the house price. For the categorical features which takes on values 0 and 1, the coefficients are given by
```{R, echo=FALSE}
cat("Coefficients of categorical features")
fit$coefficients[c("waterfront", "addhalffloor1", "basement1", "renovated1")]
```
The coefficients of "waterfront", "addhalffloor", "basement" and "renovated" is positive because they are all improvement of the house. The comparsion of magnitudes of coefficients is meanningful for the categorical features which takes on values 0 and 1. we have the order of magnitudes of coefficients given by "waterfront" > "renovated" > "basement" > "addhalffloor" which indicates their important in determining the house price.


##4. Result II: Boosting the House Price 

Here, we are going to suggest methods to boost the house price. We explore if there is any difference in the mean of logged prices in different groups for the categorical features "floors", "view", "condition" and "grade". A pairwise comparision is applied  to determine this. From the pairwise comparision, we are going to draw conclusion and recommendation to increase the house price.

In the following, we give the box plot of the logged price against the feature of interest, in which the red dot indicates the mean logged price for each group. Each box plot is accompanied with a visualization of the confidence intervals for the Tukey's honest significant difference in group mean. Conclusion and recommendation are draw from the statistical analysis.

1. "floors": From the boxplot, we can see that the group mean goes up from "floors"=1 to "floors"=2 but goes down from "floors"=2 to "floors"=3. Such trends of group mean is further confirmed by the plot of confidence intervals. The data suggests that increasing the number of floors does not generally increase the house price. While increasing from "floors"=1 to "floors"=2 boost the house price, increasing from "floors"=2 to "floors"=3 does not.

```{R, fig.width = 10, fig.height = 6, echo=FALSE, echo=FALSE}
fit = lm(log(price)~floors,data=data)
CI = TukeyHSD(aov(fit))
par(mfrow=c(1,2))
boxplot(log(price)~floors, data=data, xlab = "floors", ylab = "log(price)",main="log(price) vs floors" ,pch=20)
means = tapply(log(data$price),data$floors,mean)
points(means,col="red",pch=18)
plot(CI)
```

2. "view": From the box plot, we see that the mean of logged price increases generally as the house got more viewed. But the mean of logged price stays almost the same from "view"=1 to "view"=2. The plot of confidence intervals shows that while there is a jump in the price from "view"=0 to "view"=1, there is no significant difference in the mean from "view"=1 to "view"=2. So to boost the house price, the house should at least be viewed once. Viewing the house two times and three times does not leads to a significant change in the house price.

```{R, fig.width = 10, fig.height = 6, echo=FALSE, echo=FALSE}
fit = lm(log(price)~view,data=data)
CI = TukeyHSD(aov(fit))
par(mfrow=c(1,2))
boxplot(log(price)~view, data=data, xlab = "view", ylab = "log(price)",main="log(price) vs view" ,pch=20)
means = tapply(log(data$price),data$view,mean)
points(means,col="red",pch=18)
plot(CI)
```

3. "condition": The box plot shows that the mean of logged price increases generally if the house has a better condition. However, the mean price stays almost the same from "condition"=1 to "condition"=2. The confidence intervals on the pairwise differences also suggest that the groups "condition"=1 and "condition"=2 do not lead to a significant different in the mean price, whereas there is a significant improve in the mean price from "condition"=2 and "condition"=3. So to boost the price by increasing the "condition", the "condition" should at least be increased to 3.

```{R, fig.width = 10, fig.height = 6, echo=FALSE, echo=FALSE}
fit = lm(log(price)~condition,data=data)
CI = TukeyHSD(aov(fit))
par(mfrow=c(1,2))
boxplot(log(price)~condition, data=data, xlab = "condition", ylab = "log(price)",main="log(price) vs condition" ,pch=20)
means = tapply(log(data$price),data$condition,mean)
points(means,col="red",pch=18)
plot(CI)
```

4. "grade":  From the box plot, we see that the mean of logged price generally increasesas the house got more viewed. If we look further into the confidence interval for the Tukey's honest significant difference, we see that actually the mutual difference in the mean for "grade" = 1 to 5 are not significant. Hence grade 1 to 5 do not make significant change in the house price.  To boost the house price, the house should be at least of grade 6.

```{R, fig.width = 10, fig.height = 6, echo=FALSE, echo=FALSE}
fit = lm(log(price)~grade,data=data)
CI = TukeyHSD(aov(fit))
par(mfrow=c(1,2))
boxplot(log(price)~grade, data=data, xlab = "grade", ylab = "log(price)",main="log(price) vs grade" ,pch=20)
means = tapply(log(data$price),data$grade,mean)
points(means,col="red",pch=18)
plot(CI)
```

#5. Result III: Specific recommendations for selling house in affluent and poor areas 

Our main object of analysis in this section is to suggest factors that can help one increase the selling price of a house in affluent and poor areas. Our assumption is that rich neighbourhoods and poor neighbourhoods might have different factors affecting the selling price of the house. We identify rich and poor neighbourhoods depending on the average price of the zipcodes. The zipcodes with top five average prices were considered as "rich neighbourhoods" and the zipcodes with least five zipcodes were considered as "poor neighbourhoods". We first look into the "grade" of the house. The below is a the brief description on how the house is graded.

1-3 Falls short of minimum building standards. Normally cabin or inferior structure.

4 Generally older, low quality construction. Does not meet code.

5 Low construction costs and workmanship. Small, simple design.

6 Lowest grade currently meeting building code. Low quality materials and simple designs.

7 Average grade of construction and design. Commonly seen in plats and older sub-divisions.

8 Just above average in construction and design. Usually better materials in both the exterior and interior finish work.

9 Better architectural design with extra interior and exterior design and quality.

10 Homes of this quality generally have high quality features. Finish work is better and more design quality is seen in the floor plans. Generally have a larger square footage.

11 Custom design and higher quality finish work with added amenities of solid woods, bathroom fixtures and more luxurious options.

12 Custom design and excellent builders. All materials are of the highest quality and all conveniences are present.

13 Generally custom designed and built. Mansion level. Large amount of highest quality cabinet work, wood trim, marble, entry ways etc. 


```{r, echo=FALSE}
aggregate_zipcode1 = data_bedroom %>% group_by(data_bedroom$zipcode) %>% summarise(price = mean(price))
colnames(aggregate_zipcode1) = c("zipcode", "price")

aggregate_zipcode2 = aggregate_zipcode1[order(aggregate_zipcode1$price),] 

aggregate_zipcode3 = aggregate_zipcode2[c(1,2,3,4,5,66,67,68,69,70),]

print(c("98002", "98168", "98032", "98001", "98148"))
print(c("98102", "98112", "98040", "98004", "98039"))
```


```{r, fig.width = 10, fig.height = 6, echo=FALSE}
data_poor = subset(data_bedroom , zipcode %in% c("98002", "98168", "98032", "98001", "98148")) 
data_rich = subset(data_bedroom , zipcode %in% c("98102", "98112", "98040", "98004", "98039"))

par(mfrow = c(1,2))
fit = lm(log(price)~grade,data=data_rich)
result = TukeyHSD(aov(fit))
plot(result)

fit = lm(log(price)~grade,data=data_poor)
result = TukeyHSD(aov(fit))
plot(result)


```

We perform a one way ANOVA on the data from both the regions. Although, the "grade" of a house is significant, we wanted to explore if there was any significant difference in the mean prices of houses for different grades and if this difference was similar for rich and poor neighbourhoods.

Key inferences :

1. Firstly, none of the houses in both the rich and poor neighbourhoods have houses rated between 1 and 3. This makes sense as any house that falls short of minimum building standards isnt being sold in the market irrespective of the neighbourhood.

2. It is interesting to note that although the houses with grade 4 and 5 have been sold in the poor neighbourhoods, there werent any houses which were sold with similar grades in the rich neighbourhood. We can fairly assume that the buildings with this grade which are generally older, low quality construction and simple design and are most likely to be located in the poor neighbourhoods. It makes sense that rich neighbourhoods wouldnt have any houses with this grade.

3. Also, according the results from the Tukey's plot, it doesnt matter if your house grade is 4 or 5 in the poor neighbourhoods. They are likely going to be sold for a similar price probably because they are equally bad. However, if try to get the grade to 5, ther is a significant increase in the price of your house.

4. If you want to make a better deal if your house is in the poor neighbourhood, getting to improve the grade from 9 to 10 alone will not lead to higher selling price. Improving the grade from 9 to 11 or 10 to 11 is more likely to lead to a significant increase in the selling price.

5. In the rich neighbourhood, the scenario is different. The selling price is not going to increase significantly from 6 to 7 i.e from "average grade" to an "above average grade". However, improving the grade from 6 to 8 or 7 to 8 is more likely to lead to a significant increase in the selling price. We can fairly conclude that if people are going to invest in the rich neighbourhood areas, they might as well see more than just an increase from "average" to "above average"

#R Shiny UI Dashboard

The following is the link to the interactive dashboard that has been constructed in R Shiny. 
https://sreekanth29.shinyapps.io/kings_county_analysis/
It contains three tabs - "Graphs", "Data" and "Maps". In the first tab "graphs", we have two graphs - the first one is a trend graph between the average price and different variables and the second shows a box plot between price and different variables. The second tab "Data" contains two sections - the first one shows all the observations based on the selections that have been made and the second shows the average price againt the variable selected. The third tab "map" shows the map of the king's county with the circles represting different houses in the county which are present in the King's County dataset. All the three componenets can be subsetted according the five variables - view, condition, floors, month, sqft_living and zip code. 

#Conclusion
From the first part of our analysis we conclude that 
